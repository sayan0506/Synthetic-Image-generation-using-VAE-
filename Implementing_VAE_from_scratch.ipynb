{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementing VAE from scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOdZt8hazVb1amCh7Q5pYyC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayan0506/Synthetic-Image-generation-using-VAE-/blob/main/Implementing_VAE_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGlMC669YQYP"
      },
      "source": [
        "# Importing Modules\r\n",
        "\r\n",
        "Importing nercessary modules, here we will implement the VAE using the tensorflow backend from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvmccuXcDHDV"
      },
      "source": [
        "import numpy as np\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubg4f-J2aA8r"
      },
      "source": [
        "# Defining Parameters\r\n",
        "\r\n",
        "Here we are defining the parameters and necessary initialization of the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trzj96S6Z3RY"
      },
      "source": [
        "# parameters of the neural network\r\n",
        "\r\n",
        "# defining the learning rate, epochs, batch_size(no. of datapoints in each mini_batch) for optimization\r\n",
        "learning_rate = 0.001\r\n",
        "epochs = 30000\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# network parameters\r\n",
        "# we have image size 28*28\r\n",
        "# we will flatten the images as 784 features of input to the neural network\r\n",
        "image_dimension = 784\r\n",
        "# dimension of the hidden layer, whhich will be same shape for both encoder and decoder layer\r\n",
        "hidden_dimension = 512\r\n",
        "\r\n",
        "# inititally we are taking latent space dimension as 2\r\n",
        "latent_space_dimension = 2\r\n",
        "\r\n",
        "# here we are defining the xavier-initialization parameters, which helps to prevent the error due to vanishing and exploding gradients for deep networks\r\n",
        "def xavier(in_shape):\r\n",
        "  # here we are restricting the standard deviations of the distribution of the weights in a certain limit which is 1/sqrt(n/2)\r\n",
        "  # where n is the number of neurons in the layer\r\n",
        "  # tf.random.normal() creates a matrix whose elements are sampled from a normal distribution having stad. as mentioned\r\n",
        "  val = tf.random.normal(shape = in_shape, stddev  = 1./tf.sqrt(in_shape[0]/2.))\r\n",
        "  # use tf.random.normal() instead of tf.random_normal(), else it will throw module not found error\r\n",
        "  tf.random_normal_initializer()\r\n",
        "  return val\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRYVKg5aYXpk"
      },
      "source": [
        "Here we are defining the weights and bias dictionaries corresponding to the VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6wGFg51b24r"
      },
      "source": [
        "# weight dictionary corresponding to the 5 layers\r\n",
        "# note if we take tf.Variable(), we have to initialize it, else \r\n",
        "# in each layer the regression equation is [z = transpose(W) + b]\r\n",
        "weight = {\"weight_matrix_encoder_hidden\": tf.Variable(xavier([image_dimension, hidden_dimension])),\r\n",
        "          \"weight_matrix_mean_hidden\": tf.Variable(xavier([hidden_dimension, latent_space_dimension])),\r\n",
        "          \"weight_matrix_std_hidden\": tf.Variable(xavier([hidden_dimension, latent_space_dimension])),\r\n",
        "          \"weight_matrix_decoder_hidden\": tf.Variable(xavier([latent_space_dimension, hidden_dimension]))\r\n",
        "          }\r\n",
        "\r\n",
        "# defining the bias dictionary\r\n",
        "bias = {\"bias_matrix_encoder_hidden\": tf.Variable(xavier([image_dimension, hidden_dimension])),\r\n",
        "        \"bias_matrix_mean_hidden\": tf.Variable(xavier([hidden_dimension, latent_space_dimension])),\r\n",
        "        \"bias_matrix_std_hidden\": tf.Variable(xavier([hidden_dimension, latent_space_dimension])),\r\n",
        "        \"bias_matrix_decoder_hidden\": tf.Variable(xavier([latent_space_dimension, hidden_dimension]))\r\n",
        "        }"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WEUmUf7ZpP7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}